{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fwei/miniconda3/envs/pointpillars/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"OMP_NUM_THREADS\"]=\"1\"\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import glob\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time\n",
    "import pickle\n",
    "exp_name = 'track_seq4_vin_intsum_predv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_max = 0\n",
    "v_min = 0\n",
    "def read_data_n_label(data, label):\n",
    "    global v_max, v_min\n",
    "    input_data = np.load(data)[:,[0,1,3,2,9]]\n",
    "    input_data[:,2] = np.log(input_data[:,2])/16.\n",
    "    input_data = input_data[input_data[:,0] > 0]\n",
    "    input_data = input_data[input_data[:,0] < 60]\n",
    "    input_data = input_data[input_data[:,1] > -40]\n",
    "    input_data = input_data[input_data[:,1] < 40]\n",
    "\n",
    "    grid_in = np.zeros((1,600,800,2))\n",
    "    count_in = np.ones((1,600,800,1))\n",
    "\n",
    "    for inz in input_data:\n",
    "        pos_x = int(inz[0]//0.1)\n",
    "        pos_y = int((inz[1] + 40)//0.1)\n",
    "        grid_in[0, pos_x, pos_y, 0] += inz[2]#(grid_in[0, pos_x, pos_y, 0] + inz[2])#/(count_in[0, pos_x, pos_y, 0])\n",
    "        grid_in[0, pos_x, pos_y, 1] = (grid_in[0, pos_x, pos_y, 1]*(count_in[0, pos_x, pos_y, 0]-1) + inz[3])/count_in[0, pos_x, pos_y, 0]\n",
    "        count_in[0, pos_x, pos_y, 0] += 1\n",
    "\n",
    "    with open(label) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    grid_label = np.zeros((1,300,400,8))\n",
    "    if len(content) == 0:\n",
    "        return grid_in, grid_label, np.array([-1,-1,-1,-1])\n",
    "    velocity = (input_data[:,3][input_data[:,-1]>=0])\n",
    "    velocity = velocity.mean() if velocity.size>0 else np.zeros(1)\n",
    "    velocity = velocity/4\n",
    "    tar_location = np.fromstring(content[1][1:-1], dtype= float, sep = ' ')\n",
    "    tar_WL = np.fromstring(content[2][1:-1], dtype= float, sep = ',')\n",
    "    tar_angle = float(content[3])\n",
    "\n",
    "    pos0 = int((tar_location[0])//0.2)\n",
    "    pos1 = int((tar_location[1] + 40)//0.2)\n",
    "\n",
    "    grid_label[0, pos0, pos1, -1] = 1\n",
    "    grid_label[0, pos0, pos1, -2] = tar_angle\n",
    "    grid_label[0, pos0, pos1, 3] = tar_WL[0]\n",
    "    grid_label[0, pos0, pos1, 4] = tar_WL[1]\n",
    " \n",
    "    \n",
    "    if tar_WL[0] < tar_WL[1]:\n",
    "        grid_label[0, pos0, pos1, 3] = tar_WL[1]\n",
    "        grid_label[0, pos0, pos1, 4] = tar_WL[0]\n",
    "        grid_label[0, pos0, pos1, -2] = tar_angle + np.pi/2\n",
    "#         print ('ALARM')\n",
    "        \n",
    "    grid_label[0, pos0, pos1, 1] = tar_location[0] - (pos0*0.2) - 0.1\n",
    "    grid_label[0, pos0, pos1, 2] = tar_location[1] - (pos1*0.2 -40) -0.1\n",
    "    grid_label[0, pos0, pos1, 5] = velocity\n",
    "    if content[0] == 'bike':\n",
    "        grid_label[0, pos0, pos1, 0]  = 1\n",
    "    elif content[0] == 'pedestrian':\n",
    "        grid_label[0, pos0, pos1, 0]  = 2\n",
    "    else:\n",
    "        grid_label[0, pos0, pos1, 0]  = 0\n",
    "    if grid_label[0, pos0, pos1, -2]>(v_max):\n",
    "#         v_max=grid_label[0, pos0, pos1, -2]\n",
    "        v_max=grid_in[0, :, :, 0].max()\n",
    "    if grid_label[0, pos0, pos1, -2]<(v_min):\n",
    "#         v_min=grid_label[0, pos0, pos1, -2]\n",
    "        v_min=grid_in[0, :, :, 0].min()\n",
    "    return grid_in, grid_label, np.array([tar_location[0], tar_location[1], tar_WL[0], tar_WL[1]])\n",
    "\n",
    "\n",
    "# X.shape, y[:,:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folders = glob.glob('training_data/*')\n",
    "training_labels = glob.glob(training_folders[0] + '/labels/radar_left_np/*')\n",
    "training_X = glob.glob(training_folders[0] + '/radar_left_np/*')\n",
    "training_labels.sort()\n",
    "training_X.sort()\n",
    "X = []\n",
    "y = []\n",
    "for j in range(len(training_folders)):\n",
    "    training_labels = glob.glob(training_folders[j] + '/labels/radar_left_np/*')\n",
    "    training_X = glob.glob(training_folders[j] + '/radar_left_np/*')\n",
    "#     print (j,training_folders[j])\n",
    "    for i in range(len(training_labels)):\n",
    "        X_in, y_in, location = read_data_n_label(training_X[i], training_labels[i])\n",
    "        X.append(X_in)\n",
    "        y.append(y_in)\n",
    "X = np.array(X)\n",
    "X = X.reshape(len(X),600,800,2)\n",
    "y = np.array(y)\n",
    "y = y.reshape(len(y),300,400,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(image_in):\n",
    "    \n",
    "    with tf.variable_scope('feature_extractor'):\n",
    "        net = tf.layers.conv2d(image_in, 8, 3,strides=2, activation=tf.nn.relu, padding='same')\n",
    "        net = tf.layers.conv2d(net, 8, 3, strides=1,activation=tf.nn.relu, padding='same')\n",
    "        net = tf.layers.conv2d(net, 8, 3, strides=1,activation=tf.nn.relu, padding='same')\n",
    "\n",
    "        x1 = net\n",
    "        net = tf.layers.conv2d(net, 16, 3,strides=2, activation=tf.nn.relu, padding='same')\n",
    "        net = tf.layers.conv2d(net, 32, 3, strides=1,activation=tf.nn.relu, padding='same')\n",
    "        net = tf.layers.conv2d(net, 32, 3, strides=1,activation=tf.nn.relu, padding='same')\n",
    "\n",
    "        x2 = net\n",
    "    return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-50d746723e67>:4: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /home/fwei/miniconda3/envs/pointpillars/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-12a53a4b2b91>:22: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2DTranspose` instead.\n"
     ]
    }
   ],
   "source": [
    "images_in = tf.placeholder(tf.float32, (None,600,800,2,4))\n",
    "y_label = tf.placeholder(tf.int32, (None,300, 400, 4, 1 ))\n",
    "y_label_one_hot = tf.one_hot(y_label, 3, axis=-1)\n",
    "y_angle = tf.placeholder(tf.float32, (None,300, 400 , 4, 1))\n",
    "y_wl = tf.placeholder(tf.float32, (None,300, 400 , 4, 2))\n",
    "y_loc = tf.placeholder(tf.float32, (None,300, 400 , 4, 2))\n",
    "y_v = tf.placeholder(tf.float32, (None,300, 400 , 4, 1))\n",
    "\n",
    "y_mask = tf.placeholder(tf.float32, (None,300, 400 , 4))\n",
    "\n",
    "with tf.variable_scope(\"features\") as scope:\n",
    "    x11, x12 = feature_extractor(images_in[:,:,:,:,0])\n",
    "    scope.reuse_variables()\n",
    "    x21, x22 = feature_extractor(images_in[:,:,:,:,1])\n",
    "    scope.reuse_variables()\n",
    "    x31, x32 = feature_extractor(images_in[:,:,:,:,2])\n",
    "    scope.reuse_variables()\n",
    "    x41, x42 = feature_extractor(images_in[:,:,:,:,3])\n",
    "    \n",
    "x1 = tf.concat([x11, x21, x31, x41], axis = -1)\n",
    "x2 = tf.concat([x12, x22, x32, x42], axis = -1)\n",
    "up1 = tf.layers.conv2d_transpose(x1, 32, 3, activation=tf.nn.relu, padding='same')\n",
    "up2 = tf.layers.conv2d_transpose(x2, 64, 3,strides=2, activation=tf.nn.relu, padding='same')\n",
    "\n",
    "concat = tf.concat([up1, up2], axis = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_head(concat_vec, y_label_one_hot_ts, y_angle_ts, y_loc_ts, y_wl_ts, y_v_ts, y_mask_ts, name):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        y_pred_occ = tf.layers.conv2d(concat_vec, 3 , 1, activation=None, padding= 'same')\n",
    "        y_pred_occ_prob = tf.nn.softmax(y_pred_occ, axis = -1)\n",
    "        y_pred_angle = tf.layers.conv2d(concat_vec, 1 , 1, activation=None, padding= 'same')\n",
    "        y_pred_loc   = tf.layers.conv2d(concat_vec, 2 , 1, activation=None, padding= 'same')\n",
    "        y_pred_WL   = tf.layers.conv2d(concat_vec, 2 , 1, activation=tf.nn.softplus, padding= 'same')\n",
    "        y_pred_v = tf.layers.conv2d(concat_vec, 1 , 1, activation=None, padding= 'same')\n",
    "        \n",
    "        \n",
    "        log_y_pred_WL = tf.log(y_pred_WL + 0.1)\n",
    "        log_y_wl = tf.log(y_wl_ts + 0.1)\n",
    "\n",
    "        loss_occ = tf.reduce_mean(-y_label_one_hot_ts[:,:, : , 0] *0.008*((1 - y_pred_occ_prob[:,:,:,0])) * tf.log(y_pred_occ_prob[:,:,:,0]  + 0.01 )\\\n",
    "                             -y_label_one_hot_ts[:,:, : , 1] *0.992*((1 - y_pred_occ_prob[:,:,:,1])) * tf.log(y_pred_occ_prob[:,:,:,1] + 0.01)\\\n",
    "                              -y_label_one_hot_ts[:,:, : , 2] *0.992*((1 - y_pred_occ_prob[:,:,:,2])) * tf.log(y_pred_occ_prob[:,:,:,2] + 0.01)\\\n",
    "                             )\n",
    "        loss_angle = tf.reduce_mean(y_mask_ts*tf.abs(tf.sin(y_pred_angle - y_angle_ts)))\n",
    "        loss_loc = tf.reduce_mean(y_mask_ts*tf.abs(y_loc_ts- y_pred_loc))\n",
    "        loss_wl = tf.reduce_mean(y_mask_ts*tf.abs(log_y_wl - log_y_pred_WL))\n",
    "        loss_v = tf.reduce_mean(y_mask_ts*tf.abs(y_v_ts - y_pred_v)*(y_v_ts!=0))\n",
    "\n",
    "\n",
    "        total_loss = 1.0*loss_occ + 1.25*loss_angle + 0.75*loss_loc + loss_wl + 1 * loss_v\n",
    "        \n",
    "    return y_pred_occ_prob, y_pred_angle, y_pred_loc, y_pred_WL, y_pred_v, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_occ_prob0, y_pred_angle0, y_pred_loc0, y_pred_WL0, y_pred_v0, total_loss0 = prediction_head(concat, y_label_one_hot[:,:,:,0,0,:], y_angle[:,:,:,0,:], \\\n",
    "                y_loc[:,:,:,0,:], y_wl[:,:,:,0,:], y_v[:,:,:,0,:], tf.expand_dims(y_mask[:,:,:,0], axis = -1), 'prediction_0')\n",
    "\n",
    "y_pred_occ_prob1, y_pred_angle1, y_pred_loc1, y_pred_WL1, y_pred_v1, total_loss1 = prediction_head(concat, y_label_one_hot[:,:,:,1,0,:], y_angle[:,:,:,1,:], \\\n",
    "                y_loc[:,:,:,1,:], y_wl[:,:,:,1,:], y_v[:,:,:,1,:], tf.expand_dims(y_mask[:,:,:,1], axis = -1), 'prediction_1')\n",
    "\n",
    "y_pred_occ_prob2, y_pred_angle2, y_pred_loc2, y_pred_WL2, y_pred_v2, total_loss2 = prediction_head(concat, y_label_one_hot[:,:,:,2,0,:], y_angle[:,:,:,2,:], \\\n",
    "                y_loc[:,:,:,2,:], y_wl[:,:,:,2,:], y_v[:,:,:,2,:], tf.expand_dims(y_mask[:,:,:,2], axis = -1), 'prediction_2')\n",
    "\n",
    "y_pred_occ_prob3, y_pred_angle3, y_pred_loc3, y_pred_WL3, y_pred_v3, total_loss3 = prediction_head(concat, y_label_one_hot[:,:,:,3,0,:], y_angle[:,:,:,3,:], \\\n",
    "                y_loc[:,:,:,3,:], y_wl[:,:,:,3,:], y_v[:,:,:,3,:], tf.expand_dims(y_mask[:,:,:,3], axis = -1), 'prediction_3')\n",
    "\n",
    "total_loss = total_loss0 + 0.75*total_loss1 + 0.5*total_loss2 + 0.25*total_loss3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_min,v_max #angle(-1.5687682499618538, 1.5707963267948966)\n",
    "#velocity (-4.0950820584830465, 4.674418471324403)\n",
    "#intensity (0.0, 4.248383107944953)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_seq_data(X, Y):\n",
    "    b_x = []\n",
    "    b_y = []\n",
    "    Y = np.expand_dims(Y, axis = 3)\n",
    "    X = np.expand_dims(X, axis = -1)\n",
    "    for i in range(len(X) -6):\n",
    "        b_x.append(np.concatenate((X[i], X[i+1], X[i+2], X[i+3]), axis = -1))\n",
    "        b_y.append(np.concatenate((Y[i+3], Y[i+4], Y[i+5], Y[i+6]), axis = 2))\n",
    "    \n",
    "    b_x = np.asarray(b_x)\n",
    "    b_y = np.asarray(b_y)\n",
    "    \n",
    "    return b_x, b_y\n",
    "\n",
    "def dataset(np_seed, seqlen=10):\n",
    "    Xs = []\n",
    "    labels = []\n",
    "    training_folders = glob.glob('training_data/*')\n",
    "    for j in range(len(training_folders)):\n",
    "        training_labels = glob.glob(training_folders[j] + '/labels/radar_left_np/*')\n",
    "        training_X = glob.glob(training_folders[j] + '/radar_left_np/*')\n",
    "        training_labels.sort()\n",
    "        training_X.sort()\n",
    "        i_begin = np.random.RandomState(np_seed).randint(seqlen)\n",
    "        training_labels = training_labels[i_begin:]\n",
    "        training_X = training_X[i_begin:]\n",
    "        n_seq = len(training_X)//seqlen\n",
    "        i_end = n_seq * seqlen\n",
    "        training_labels = training_labels[:i_end]\n",
    "        training_X = training_X[:i_end]\n",
    "#         print(len(training_labels))\n",
    "        Xs.extend(training_X)\n",
    "        labels.extend(training_labels)\n",
    "    return Xs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18670\n",
      "epoch 0\n",
      "cost:  6.0746996e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([58, 79, 79, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81, 82, 82, 82,\n",
      "       83]), array([273, 247, 248, 246, 247, 248, 249, 250, 247, 248, 249, 250, 251,\n",
      "       252, 250, 251, 252, 251]))\n",
      "labels: (array([81]), array([250]))\n",
      "Pre_ID:  3588 , GT_ID:  254728\n",
      "Time Elapsed:  1147.4080407619476\n",
      "18830\n",
      "epoch 1\n",
      "cost:  4.9882423e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([22, 35, 36, 36, 79, 79, 79, 79, 80, 80, 80, 80, 80, 80, 80, 81, 81,\n",
      "       81, 81, 81, 81, 82, 82, 82, 82, 86, 87, 87]), array([211, 202, 202, 203, 243, 244, 248, 249, 243, 244, 245, 248, 249,\n",
      "       250, 251, 244, 248, 249, 250, 251, 252, 250, 251, 252, 253, 255,\n",
      "       255, 256]))\n",
      "labels: (array([81]), array([248]))\n",
      "Pre_ID:  355206 , GT_ID:  261182\n",
      "Time Elapsed:  1143.0243866443634\n",
      "18360\n",
      "epoch 2\n",
      "cost:  5.5031196e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([ 51,  51,  52,  52,  52,  52,  53,  53,  53,  78,  79,  79,  79,\n",
      "        79,  79,  79,  79,  79,  79,  79,  80,  80,  80,  80,  80,  80,\n",
      "        80,  80,  80,  80,  81,  81,  81, 100]), array([270, 271, 270, 271, 272, 273, 270, 271, 272, 240, 238, 239, 240,\n",
      "       241, 242, 243, 244, 245, 246, 247, 239, 240, 241, 242, 243, 244,\n",
      "       245, 246, 247, 248, 246, 247, 249, 252]))\n",
      "labels: (array([79]), array([245]))\n",
      "Pre_ID:  355206 , GT_ID:  254739\n",
      "Time Elapsed:  1113.2589979171753\n",
      "18530\n",
      "epoch 3\n",
      "cost:  4.626767e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([52, 78, 79, 79, 79, 80, 80, 80, 81, 81, 81]), array([207, 241, 240, 241, 242, 241, 248, 249, 248, 249, 250]))\n",
      "labels: (array([79]), array([241]))\n",
      "Pre_ID:  69036 , GT_ID:  254766\n",
      "Time Elapsed:  1131.2971694469452\n",
      "18360\n",
      "epoch 4\n",
      "cost:  4.5472192e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([ 51,  51,  52,  52,  52,  52,  78,  78,  78,  79,  79,  79,  79,\n",
      "        79,  79,  79,  79,  80,  80,  80,  80,  80,  80,  81,  81, 100]), array([270, 271, 270, 271, 272, 273, 243, 244, 245, 240, 241, 242, 243,\n",
      "       244, 245, 246, 247, 243, 244, 245, 246, 247, 248, 246, 247, 252]))\n",
      "labels: (array([79]), array([245]))\n",
      "Pre_ID:  119574 , GT_ID:  254739\n",
      "Time Elapsed:  1130.9843211174011\n",
      "18670\n",
      "epoch 5\n",
      "cost:  5.76005e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([58, 58, 58, 59, 59, 79, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81,\n",
      "       82, 82, 82]), array([272, 273, 274, 273, 274, 246, 246, 247, 248, 249, 250, 246, 247,\n",
      "       248, 249, 250, 251, 249, 250, 251]))\n",
      "labels: (array([81]), array([250]))\n",
      "Pre_ID:  101538 , GT_ID:  254728\n",
      "Time Elapsed:  1150.6053993701935\n",
      "18440\n",
      "epoch 6\n",
      "cost:  4.8450693e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([52, 52, 53, 54, 54, 79, 79, 79, 79, 79, 79, 80, 80, 80, 80, 80, 80,\n",
      "       81, 81, 81]), array([273, 274, 272, 271, 272, 244, 245, 246, 247, 248, 249, 241, 245,\n",
      "       246, 247, 248, 249, 248, 249, 250]))\n",
      "labels: (array([80]), array([244]))\n",
      "Pre_ID:  8151 , GT_ID:  251542\n",
      "Time Elapsed:  1147.864646434784\n",
      "18620\n",
      "epoch 7\n",
      "cost:  4.8549642e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([79, 79, 79, 80, 80, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81,\n",
      "       81, 81, 81, 82, 82, 82, 86]), array([246, 247, 248, 243, 244, 245, 246, 247, 248, 249, 250, 243, 244,\n",
      "       245, 246, 247, 248, 249, 250, 251, 249, 250, 251, 255]))\n",
      "labels: (array([81]), array([247]))\n",
      "Pre_ID:  103935 , GT_ID:  258000\n",
      "Time Elapsed:  1150.662647485733\n",
      "18330\n",
      "epoch 8\n",
      "cost:  4.1141906e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([50, 50, 51, 51, 52, 52, 56, 77, 78, 78, 78, 78, 78, 78, 78, 79, 79,\n",
      "       79, 79, 79, 79, 80, 80, 80, 80, 80, 81, 81, 81]), array([271, 272, 271, 272, 272, 273, 277, 242, 239, 240, 241, 242, 243,\n",
      "       244, 245, 241, 242, 244, 245, 246, 247, 245, 246, 247, 248, 249,\n",
      "       247, 248, 249]))\n",
      "labels: (array([79]), array([245]))\n",
      "Pre_ID:  357588 , GT_ID:  254742\n",
      "Time Elapsed:  1078.858959197998\n",
      "18620\n",
      "epoch 9\n",
      "cost:  5.657379e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([79, 79, 80, 80, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81, 81,\n",
      "       81, 82, 82]), array([246, 248, 243, 244, 245, 246, 247, 248, 249, 250, 243, 244, 245,\n",
      "       246, 247, 248, 249, 250, 249, 251]))\n",
      "labels: (array([81]), array([247]))\n",
      "Pre_ID:  357594 , GT_ID:  258000\n",
      "Time Elapsed:  1034.8684260845184\n",
      "18360\n",
      "epoch 10\n",
      "cost:  3.9742816e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([ 51,  51,  51,  52,  52,  52,  52,  53,  53,  78,  78,  78,  79,\n",
      "        79,  79,  79,  79,  79,  79,  79,  79,  80,  80,  80,  80,  80,\n",
      "        80,  80,  81,  81,  81,  81, 100]), array([270, 271, 272, 270, 271, 272, 273, 272, 273, 242, 243, 244, 240,\n",
      "       241, 242, 243, 244, 245, 246, 247, 248, 243, 244, 245, 246, 247,\n",
      "       248, 249, 246, 247, 248, 249, 252]))\n",
      "labels: (array([79]), array([245]))\n",
      "Pre_ID:  359994 , GT_ID:  254739\n",
      "Time Elapsed:  1023.1888647079468\n",
      "18530\n",
      "epoch 11\n",
      "cost:  4.767946e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([52, 79, 79, 79, 80, 80, 80, 80, 81, 81, 81]), array([207, 241, 242, 247, 241, 242, 248, 249, 248, 249, 250]))\n",
      "labels: (array([79]), array([241]))\n",
      "Pre_ID:  69018 , GT_ID:  254766\n",
      "Time Elapsed:  1035.9109382629395\n",
      "18530\n",
      "epoch 12\n",
      "cost:  4.0986146e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([52, 78, 79, 79, 79, 80, 80, 80, 80, 80, 81]), array([207, 242, 240, 241, 242, 240, 241, 242, 248, 249, 249]))\n",
      "labels: (array([79]), array([241]))\n",
      "Pre_ID:  59430 , GT_ID:  254766\n",
      "Time Elapsed:  1034.4912779331207\n",
      "18620\n",
      "epoch 13\n",
      "cost:  4.4045893e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([79, 79, 79, 80, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81, 81,\n",
      "       82, 82, 84]), array([245, 246, 247, 244, 245, 246, 247, 248, 249, 250, 244, 245, 246,\n",
      "       247, 248, 249, 250, 250, 251, 253]))\n",
      "labels: (array([81]), array([247]))\n",
      "Pre_ID:  102738 , GT_ID:  258000\n",
      "Time Elapsed:  1038.3392734527588\n",
      "18530\n",
      "epoch 14\n",
      "cost:  5.2796986e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([52, 79, 79, 79, 80, 80, 80, 80, 81]), array([207, 240, 241, 242, 240, 241, 248, 249, 249]))\n",
      "labels: (array([79]), array([241]))\n",
      "Pre_ID:  69018 , GT_ID:  254766\n",
      "Time Elapsed:  1012.8070080280304\n",
      "18670\n",
      "epoch 15\n",
      "cost:  3.6942583e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([58, 58, 58, 59, 59, 59, 80, 80, 81, 81, 81, 81, 81, 82, 82, 82, 82,\n",
      "       84]), array([273, 274, 275, 273, 274, 275, 248, 249, 247, 248, 249, 250, 251,\n",
      "       249, 250, 251, 252, 252]))\n",
      "labels: (array([81]), array([250]))\n",
      "Pre_ID:  102771 , GT_ID:  254728\n",
      "Time Elapsed:  1011.8292045593262\n",
      "18330\n",
      "epoch 16\n",
      "cost:  3.7988524e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([50, 50, 51, 51, 51, 52, 52, 77, 77, 78, 78, 78, 78, 79, 79, 79, 79,\n",
      "       79, 79, 80, 80, 80, 80, 81, 81]), array([271, 272, 271, 272, 273, 272, 273, 241, 242, 241, 242, 243, 244,\n",
      "       242, 243, 244, 245, 246, 247, 245, 246, 247, 248, 246, 248]))\n",
      "labels: (array([79]), array([245]))\n",
      "Pre_ID:  94353 , GT_ID:  254742\n",
      "Time Elapsed:  994.1344122886658\n",
      "18590\n",
      "epoch 17\n",
      "cost:  4.119146e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([79, 79, 79, 79, 80, 80, 80, 80, 81, 81, 81]), array([245, 246, 247, 248, 246, 247, 248, 249, 247, 248, 249]))\n",
      "labels: (array([80]), array([246]))\n",
      "Pre_ID:  102765 , GT_ID:  257958\n",
      "Time Elapsed:  1023.4775130748749\n",
      "18730\n",
      "epoch 18\n",
      "cost:  3.9388404e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([57, 57, 79, 79, 79, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81,\n",
      "       81, 82, 82, 86, 87]), array([273, 274, 245, 246, 247, 245, 246, 247, 248, 249, 250, 246, 247,\n",
      "       248, 249, 250, 251, 252, 251, 252, 268, 268]))\n",
      "labels: (array([81]), array([249]))\n",
      "Pre_ID:  102747 , GT_ID:  257974\n",
      "Time Elapsed:  1023.2992403507233\n",
      "18620\n",
      "epoch 19\n",
      "cost:  3.5680663e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([79, 79, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81]), array([245, 246, 244, 245, 246, 247, 248, 249, 245, 246, 247, 248, 249,\n",
      "       250]))\n",
      "labels: (array([81]), array([247]))\n",
      "Pre_ID:  102771 , GT_ID:  258000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed:  1022.4946808815002\n",
      "18440\n",
      "epoch 20\n",
      "cost:  4.436575e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([ 3, 52, 52, 53, 53, 53, 54, 54, 79, 79, 79, 79, 79, 79, 79, 79, 80,\n",
      "       80, 80, 80, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 82, 82]), array([321, 272, 273, 271, 272, 273, 271, 272, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249,\n",
      "       246, 248, 249, 250, 249, 250]))\n",
      "labels: (array([80]), array([244]))\n",
      "Pre_ID:  4581 , GT_ID:  251542\n",
      "WARNING:tensorflow:From /home/fwei/miniconda3/envs/pointpillars/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Time Elapsed:  1005.9842505455017\n",
      "18830\n",
      "epoch 21\n",
      "cost:  5.623861e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([79, 79, 79, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81, 81, 82,\n",
      "       82, 82, 83, 84, 84]), array([245, 246, 247, 245, 246, 247, 248, 249, 250, 246, 247, 248, 249,\n",
      "       250, 251, 252, 250, 251, 252, 253, 253, 254]))\n",
      "labels: (array([81]), array([248]))\n",
      "Pre_ID:  47388 , GT_ID:  261182\n",
      "Time Elapsed:  1026.3684918880463\n",
      "18670\n",
      "epoch 22\n",
      "cost:  2.5771322e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([58, 59, 59, 59, 80, 81, 81, 81, 81, 81, 82, 82, 82, 82, 83, 84]), array([274, 273, 274, 275, 248, 247, 248, 249, 250, 251, 249, 250, 251,\n",
      "       252, 252, 252]))\n",
      "labels: (array([81]), array([250]))\n",
      "Pre_ID:  78837 , GT_ID:  254728\n",
      "Time Elapsed:  1018.0339102745056\n",
      "18330\n",
      "epoch 23\n",
      "cost:  2.9780182e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([50, 51, 51, 52, 52, 56, 79, 79, 79, 79, 80, 80, 80, 80, 80, 81, 81,\n",
      "       81, 81]), array([272, 272, 273, 272, 273, 277, 244, 245, 246, 247, 245, 246, 247,\n",
      "       248, 249, 246, 247, 248, 249]))\n",
      "labels: (array([79]), array([245]))\n",
      "Pre_ID:  101526 , GT_ID:  254742\n",
      "Time Elapsed:  990.6325852870941\n",
      "18440\n",
      "epoch 24\n",
      "cost:  3.5929235e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([52, 52, 53, 53, 54, 54, 78, 79, 79, 79, 79, 79, 79, 79, 79, 80, 80,\n",
      "       80, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 82, 82]), array([272, 273, 272, 273, 271, 272, 243, 242, 243, 244, 245, 246, 247,\n",
      "       248, 249, 240, 242, 243, 244, 245, 246, 247, 248, 249, 246, 247,\n",
      "       248, 249, 250, 249, 250]))\n",
      "labels: (array([80]), array([244]))\n",
      "Pre_ID:  3381 , GT_ID:  251542\n",
      "Time Elapsed:  1048.4774622917175\n",
      "18500\n",
      "epoch 25\n",
      "cost:  3.8231996e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([53, 53, 54, 54, 79, 79, 79, 80, 80, 81, 81, 82, 82]), array([273, 274, 273, 274, 241, 248, 249, 249, 250, 249, 250, 250, 251]))\n",
      "labels: (array([80]), array([250]))\n",
      "Pre_ID:  102768 , GT_ID:  254766\n",
      "Time Elapsed:  987.4517974853516\n",
      "18620\n",
      "epoch 26\n",
      "cost:  3.8011338e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([79, 79, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81, 81, 82]), array([245, 246, 245, 246, 247, 248, 249, 250, 244, 245, 246, 247, 248,\n",
      "       249, 250, 250]))\n",
      "labels: (array([81]), array([247]))\n",
      "Pre_ID:  105171 , GT_ID:  258000\n",
      "Time Elapsed:  988.76265001297\n",
      "18620\n",
      "epoch 27\n",
      "cost:  2.1480957e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([80, 80, 80, 80, 81, 81, 81, 81, 84]), array([246, 247, 248, 249, 246, 247, 248, 249, 252]))\n",
      "labels: (array([81]), array([247]))\n",
      "Pre_ID:  101532 , GT_ID:  258000\n",
      "Time Elapsed:  995.3818295001984\n",
      "18330\n",
      "epoch 28\n",
      "cost:  2.9926505e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([50, 51, 51, 52, 52, 77, 78, 78, 79, 79, 80, 80, 80, 80]), array([272, 272, 273, 272, 273, 242, 241, 244, 245, 246, 245, 246, 247,\n",
      "       248]))\n",
      "labels: (array([79]), array([245]))\n",
      "Pre_ID:  101526 , GT_ID:  254742\n",
      "Time Elapsed:  971.051206111908\n",
      "18670\n",
      "epoch 29\n",
      "cost:  2.5581663e-05\n",
      "--------------------------------------------------------\n",
      "Predictions: (array([58, 59, 59, 80, 80, 81, 81, 81, 81, 82, 82, 82]), array([274, 274, 275, 248, 249, 248, 249, 250, 251, 250, 251, 252]))\n",
      "labels: (array([81]), array([250]))\n",
      "Pre_ID:  78837 , GT_ID:  254728\n",
      "Time Elapsed:  1059.5428187847137\n"
     ]
    }
   ],
   "source": [
    "#shuffle 1\n",
    "np_seed = 12345\n",
    "optimizer = tf.train.AdamOptimizer().minimize(total_loss)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=20)\n",
    "seqlen = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i_epoch in range(30):\n",
    "        batch_X = []\n",
    "        batch_y = []\n",
    "        t_begin = time.time()\n",
    "        Xs, labels = dataset(np_seed+i_epoch,seqlen=seqlen)\n",
    "        n_seq = len(Xs)\n",
    "        print(n_seq)\n",
    "        seq_shuffle = np.random.RandomState(np_seed+i_epoch).permutation(n_seq//seqlen)\n",
    "        for i, i_seq in enumerate(seq_shuffle):\n",
    "            for ii in range(seqlen):\n",
    "                X_in, y_in, _ = read_data_n_label(Xs[i_seq*seqlen+ii], labels[i_seq*seqlen+ii])\n",
    "#                 print (i,' ',i_seq,' ',ii,' ',i_seq*seqlen+ii)\n",
    "#                 print(Xs[i_seq*seqlen+ii])\n",
    "                batch_X.append(X_in)\n",
    "                batch_y.append(y_in)\n",
    "\n",
    "            batch_X_seq = np.array(batch_X).reshape(seqlen,600,800,2)\n",
    "            batch_y_seq = np.array(batch_y).reshape(seqlen,300,400,8)\n",
    "\n",
    "            batch_X, batch_y = processing_seq_data(batch_X_seq, batch_y_seq)\n",
    "\n",
    "            pred_occ, cost,_= sess.run([y_pred_occ_prob0,total_loss, optimizer], feed_dict = {images_in: batch_X,\\\n",
    "                                                        y_label: batch_y[:,:,:,:,0:1], \\\n",
    "                                                        y_angle: batch_y[:,:,:,:,-2:-1],\\\n",
    "                                                        y_loc: batch_y[:,:,:,:,[1,2]], \\\n",
    "                                                        y_wl: batch_y[:,:,:,:,[3,4]], \\\n",
    "                                                        y_v: batch_y[:,:,:,:,5:6],\\\n",
    "                                                        y_mask: batch_y[:,:,:,:,-1]})\n",
    "\n",
    "            if i_seq % (n_seq//4)==0:\n",
    "                print('epoch {}'.format(i_epoch))\n",
    "                print ('cost: ', cost)\n",
    "                z = pred_occ[-1]\n",
    "                argmax = z.argmax(axis = 2)\n",
    "                print ('--------------------------------------------------------')\n",
    "                print ('Predictions:',np.where(argmax > 0))\n",
    "                print ('labels:', np.where(batch_y[-1,:,:,0,-1] > 0.5))\n",
    "                print ('Pre_ID: ', np.argmax(pred_occ[-1]),', GT_ID: ', np.argmax(batch_y[-1,:,:,-1]))\n",
    "            batch_X = []\n",
    "            batch_y = []\n",
    "        save_path = saver.save(sess, \"saved_models/{}/model_e{}.ckpt\".format(exp_name, i_epoch))\n",
    "        t_elapsed = time.time()-t_begin\n",
    "        print('Time Elapsed: ', t_elapsed)\n",
    "    save_path = saver.save(sess, \"saved_models/{}/model.ckpt\".format(exp_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Testing Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_name='track-predv0'\n",
    "for i_epoch in range(11,30):\n",
    "    saver = tf.train.Saver()\n",
    "    t_begin = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, \"saved_models/{}/model_e{}.ckpt\".format(exp_name, i_epoch))\n",
    "    #     saver.restore(sess, \"saved_models/model_e{}.ckpt\".format( i_epoch))\n",
    "        print(\"Model restored.\")\n",
    "        predictions = {'metadata': [],'label_preds':[], 'scores':[], 'box3d_lidar':[],'velocity':[],'velocity_gt':[]}\n",
    "        counter = 0\n",
    "        training_folders = glob.glob('validation_data/*')\n",
    "        training_folders.sort()\n",
    "        for folder in range(len(training_folders)):\n",
    "            training_labels = glob.glob(training_folders[folder] + '/labels/radar_left_np/*')\n",
    "            print(folder)\n",
    "            training_X = glob.glob(training_folders[folder] + '/radar_left_np/*')\n",
    "            training_labels.sort()\n",
    "            training_X.sort()\n",
    "            X = []\n",
    "            y = []\n",
    "            for i in range(len(training_labels)):\n",
    "                X_in, y_in, location = read_data_n_label(training_X[i], training_labels[i])\n",
    "                X.append(X_in)\n",
    "                y.append(y_in)\n",
    "            X = np.concatenate(X, axis=0)\n",
    "            y = np.concatenate(y, axis=0)\n",
    "            X = np.concatenate([np.tile(X[:1],(6,1,1,1)),X],axis=0)\n",
    "            var_names = ['occ','angle','loc','wl', 'v']\n",
    "            for j in range(4):\n",
    "                for var in var_names:\n",
    "                    code = \"pred_{}X{}s=[]\".format(var,j)\n",
    "                    exec(code)       \n",
    "            for i in range(3,len(X)):\n",
    "                pred_occX0, pred_angleX0, pred_locX0, pred_wlX0,pred_vX0, pred_occX1, pred_angleX1, pred_locX1, pred_wlX1,pred_vX1,\\\n",
    "                pred_occX2, pred_angleX2, pred_locX2, pred_wlX2,pred_vX2, pred_occX3, pred_angleX3, pred_locX3, pred_wlX3,pred_vX3 \\\n",
    "                                                                = sess.run([y_pred_occ_prob0, y_pred_angle0, y_pred_loc0, y_pred_WL0,y_pred_v0,\\\n",
    "                                                                           y_pred_occ_prob1, y_pred_angle1, y_pred_loc1, y_pred_WL1,y_pred_v1,\\\n",
    "                                                                           y_pred_occ_prob2, y_pred_angle2, y_pred_loc2, y_pred_WL2,y_pred_v2,\\\n",
    "                                                                           y_pred_occ_prob3, y_pred_angle3, y_pred_loc3, y_pred_WL3,y_pred_v3], \\\n",
    "                                                               feed_dict = {images_in: np.stack((X[i-3],X[i-2],X[i-1],X[i]),axis=-1)[np.newaxis]})\n",
    "                for j in range(4):\n",
    "                    for var in var_names:\n",
    "                        code = \"pred_{}X{}s.append(pred_{}X{})\".format(var,j,var,j)\n",
    "                        exec(code)\n",
    "            for i in range(6, len(X)):\n",
    "                for var in var_names:\n",
    "                    for j in range(4):\n",
    "                        code = \"pred_{}X{}=pred_{}X{}s[{}]\".format(var,j,var,j,i-3-j)\n",
    "                        exec(code)\n",
    "\n",
    "                    code = \"pred_{} = np.mean(np.array([pred_{}X0, pred_{}X1, pred_{}X2, pred_{}X3]), axis = 0)\".format(var,var,var,var,var)\n",
    "                    exec(code)\n",
    "\n",
    "                pred_index = np.asarray(np.where(pred_occ[0,:,:,:].argmax(axis = -1)>0)).T\n",
    "                #pred_index = np.asarray(np.where(pred_occ[0,:,:,0] < 0.5)).T\n",
    "                iii = i -6\n",
    "                try:\n",
    "                    v_gt = y[iii][...,5].sum()\n",
    "                except:\n",
    "                    v_gt = 0\n",
    "                predictions['velocity_gt'].append(v_gt*4)\n",
    "                predictions['metadata'].append(training_X[iii])\n",
    "\n",
    "                predictions['label_preds'].append([])\n",
    "                predictions['box3d_lidar'].append([])\n",
    "                predictions['scores'].append([])\n",
    "                predictions['velocity'].append([])\n",
    "\n",
    "                for index in range(len(pred_index)):\n",
    "\n",
    "                    predictions['label_preds'][counter].append(pred_occ[:,pred_index[index,0], pred_index[index,1], [1,2]].argmax(axis = -1)[0])\n",
    "                    predictions['scores'][counter].append(pred_occ[:,pred_index[index,0], pred_index[index,1], [1,2]].max(axis = -1)[0])\n",
    "                    predictions['velocity'][counter].append(pred_v[:,pred_index[index,0], pred_index[index,1], 0]*4)\n",
    "\n",
    "                    pred_location_x =  pred_loc[:,pred_index[index,0], pred_index[index,1],0] + (pred_index[index,0]*0.2) + 0.1\n",
    "                    pred_location_y =  pred_loc[:,pred_index[index,0], pred_index[index,1],1] + (pred_index[index,1]*0.2 -40) + 0.1\n",
    "\n",
    "                    W_location_xy = pred_wl[:,pred_index[index,0], pred_index[index,1],0]\n",
    "                    L_location_xy = pred_wl[:,pred_index[index,0], pred_index[index,1],1]\n",
    "\n",
    "                    angle_xy = pred_angle[:,pred_index[index,0], pred_index[index,1],0]\n",
    "                    predictions['box3d_lidar'][counter].append([])\n",
    "                    predictions['box3d_lidar'][counter][-1].append(pred_location_x[0])\n",
    "                    predictions['box3d_lidar'][counter][-1].append(pred_location_y[0])\n",
    "                    predictions['box3d_lidar'][counter][-1].append(-1)\n",
    "                    predictions['box3d_lidar'][counter][-1].append(W_location_xy[0])\n",
    "                    predictions['box3d_lidar'][counter][-1].append(L_location_xy[0])\n",
    "                    predictions['box3d_lidar'][counter][-1].append(-1)\n",
    "                    predictions['box3d_lidar'][counter][-1].append(angle_xy[0])\n",
    "                counter += 1\n",
    "    fname = \"saved_models/{}/model_e{}.pkl\".format(exp_name, i_epoch)\n",
    "    with open(fname, 'wb') as output:\n",
    "        pickle.dump(predictions, output)\n",
    "    print(time.time()-t_begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 velocity loss:  [0.6160672]   [7.1625457]\n",
      "13 velocity loss:  [0.54587847]   [7.2101192]\n",
      "14 velocity loss:  [0.60353106]   [18.031399]\n",
      "15 velocity loss:  [0.588824]   [5.227105]\n",
      "16 velocity loss:  [0.50219256]   [7.964585]\n",
      "17 velocity loss:  [0.54366386]   [4.2573857]\n",
      "18 velocity loss:  [0.54285294]   [3.6996415]\n",
      "19 velocity loss:  [0.53239447]   [1.9758142]\n",
      "20 velocity loss:  [0.56065905]   [8.541448]\n",
      "21 velocity loss:  [0.5151367]   [2.5864663]\n",
      "22 velocity loss:  [0.5221981]   [6.7348156]\n",
      "23 velocity loss:  [0.49357244]   [4.507259]\n",
      "24 velocity loss:  [0.56932205]   [6.8083835]\n",
      "25 velocity loss:  [0.51979756]   [2.6106768]\n",
      "26 velocity loss:  [0.60385424]   [7.1143856]\n",
      "27 velocity loss:  [0.5422476]   [5.6779118]\n",
      "28 velocity loss:  [0.5536952]   [6.275417]\n",
      "29 velocity loss:  [0.52496666]   [5.7120643]\n"
     ]
    }
   ],
   "source": [
    "# save NMS prediction\n",
    "from shapely.geometry import Polygon\n",
    "def get_rect(filler):\n",
    "    W = filler[3]\n",
    "    L = filler[4]\n",
    "    angle = filler[-1]\n",
    "\n",
    "    m0 = np.array([filler[0], filler[1]])\n",
    "    k1 = np.array([np.cos(angle), np.sin(angle)])*W\n",
    "    k2 = np.array([np.sin(angle), -np.cos(angle)])*L\n",
    "    p0 = m0 - k1 - k2 # m0 + k1 + k2\n",
    "    p1 = m0 +k1 - k2 #m0 - k1 - k2\n",
    "    p2 = m0 + k1 + k2\n",
    "    p3 = m0 - k1 + k2\n",
    "    \n",
    "    return Polygon([p0, p1, p2, p3]), [p0, p1, p2, p3]\n",
    "\n",
    "# exp_name = 'track_seq4_vin_intsum_predv'\n",
    "nonobject= True #False\n",
    "for i_epoch in range(12,30):\n",
    "    fname = \"saved_models/{}/model_e{}\".format(exp_name, i_epoch)\n",
    "    with open(fname+\".pkl\", 'rb') as file:\n",
    "        object_file = pickle.load(file)\n",
    "    nms_dict = {'metadata': [],'label_preds':[], 'scores':[], 'box3d_lidar':[], 'velocity':[]}\n",
    "    v_loss = []\n",
    "    for i in range(len(object_file['metadata'])):\n",
    "\n",
    "        score = np.array(object_file['scores'][i])\n",
    "        llabel = np.array(object_file['label_preds'][i])\n",
    "        rrect  = np.array(object_file['box3d_lidar'][i])\n",
    "        velo = np.array(object_file['velocity'][i])\n",
    "        \"\"\"okay = np.where(score> 0.45)[0]\n",
    "        score = score[okay]\n",
    "        llabel = llabel[okay]\n",
    "        rrect = rrect[okay]\"\"\"\n",
    "\n",
    "        nms_dict['metadata'].append(object_file['metadata'][i])\n",
    "        nms_dict['label_preds'].append([])\n",
    "        nms_dict['box3d_lidar'].append([])\n",
    "        nms_dict['scores'].append([])\n",
    "        nms_dict['velocity'].append([])\n",
    "        if len(score)==0:\n",
    "            v_diff = object_file['velocity_gt'][i]\n",
    "        else:\n",
    "            v_diff = None\n",
    "        while(len(score) > 0):\n",
    "            nms_dict['box3d_lidar'][i].append([])\n",
    "            under_attention = score.argmax()\n",
    "            max_score = score.max()\n",
    "    #         print(max_score)\n",
    "            class_pred = llabel[under_attention]\n",
    "            #score = np.delete(score, under_attention)\n",
    "            if v_diff is None:\n",
    "                if object_file['velocity_gt'][i]==0:\n",
    "                    v_diff = 1\n",
    "                else:\n",
    "                    v_diff = (velo[under_attention] - object_file['velocity_gt'][i])/(object_file['velocity_gt'][i])\n",
    "    #         print (llabel)\n",
    "    #         print (rrect.shape)\n",
    "    #         print(score.shape)\n",
    "            box_attent, box_arr = get_rect(rrect[under_attention])\n",
    "            box_apc = rrect[under_attention]\n",
    "            #print (object_file['box3d_lidar'][i])\n",
    "            #nms_dict['box3d_lidar'][i][-1].append(box_arr)\n",
    "            box_arr = np.array(box_arr)\n",
    "            deletion_list = []\n",
    "            avg_conf= []\n",
    "            count = 1.0\n",
    "            for mmm in range(len(score)):\n",
    "                boxmm, boxyy = get_rect(rrect[mmm])\n",
    "                boxyy = np.array(boxyy)\n",
    "                if (box_attent.intersection(boxmm).area / box_attent.union(boxmm).area) > 0:\n",
    "                    if nonobject or (not nonobject and class_pred == llabel[mmm]):\n",
    "                        deletion_list.append(mmm)\n",
    "\n",
    "            nms_dict['label_preds'][i].append(class_pred)\n",
    "            nms_dict['scores'][i].append(max_score)\n",
    "            nms_dict['box3d_lidar'][i][-1].append(box_apc)\n",
    "            nms_dict['velocity'][i].append(velo[under_attention])\n",
    "            score = np.delete(score, deletion_list)\n",
    "            rrect = np.delete(rrect, deletion_list, axis = 0)\n",
    "            llabel = np.delete(llabel, deletion_list)\n",
    "        v_loss.append(v_diff)\n",
    "    v_loss = np.array(v_loss)\n",
    "    v_loss1 = np.abs(v_loss).mean() #/len(object_file['metadata'])\n",
    "    v_loss2 = (v_loss**2).mean() #/len(object_file['metadata'])\n",
    "    print(i_epoch, 'velocity loss: ', v_loss1,' ', v_loss2)\n",
    "    \n",
    "    nms_list = []\n",
    "    for i in range(len(nms_dict['metadata'])):\n",
    "        sub_dict = dict()\n",
    "        sub_dict['metadata'] = dict()\n",
    "        sub_dict['metadata']['image_idx']= nms_dict['metadata'][i]\n",
    "    #     sub_dict['metadata']['image_shape']= np.array([ 600, 800])\n",
    "        sub_dict['label_preds'] = np.array(nms_dict['label_preds'][i])\n",
    "        sub_dict['scores'] = np.array(nms_dict['scores'][i])\n",
    "        sub_dict['velocity'] = np.array(nms_dict['velocity'][i])\n",
    "        if len(nms_dict['box3d_lidar'][i]) >0:\n",
    "            sub_dict['box3d_lidar'] = np.array(nms_dict['box3d_lidar'][i]).squeeze(axis = 1)\n",
    "        else:\n",
    "            sub_dict['box3d_lidar'] = np.array(nms_dict['box3d_lidar'][i])\n",
    "        nms_list.append(sub_dict)\n",
    "    \n",
    "    if nonobject:\n",
    "        fname +='-n'\n",
    "    with open(fname+'-nms.pkl', 'wb') as output:\n",
    "        pickle.dump(nms_list, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
